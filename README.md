# Collaborative-filtering-recommendation-algorithm-based-on-kNN-model
numpy实现基于k近邻模型的协同过滤推荐算法
算法概述



推荐算法常用的有三种：基于人口统计学的推荐、基于内容的推荐、基于协同过滤的推荐。前两种是仅仅考虑用户或物品本身背景属性，将拥有相似属性的用户或物品分在一个集合里。第三种算法是在大量用户行为和数据中收集答案，以帮助对整个人群产生统计意义上的结论。此处我们重点讨论第三种，基于协同过滤的推荐算法。

协同过滤

核心是用户交互数据建模，可以分为三个子类：user-based recommendation，item-based recommendation，model-based recommendation。

基于用户的协同过滤推荐（user-based recommendation）

它的基本假设是，喜欢类似物品的用户可能有相同或者相似的口味和偏好。根据所有用户对物品或者信息的偏好，发现与当前用户口味和偏好相似的“邻居”用户群，在一般的应用中是采用计算“K- 近邻”的算法；然后，基于这 K 个邻居的历史偏好信息，为当前用户进行推荐。

这里写图片描述

基于项目的协同过滤推荐

基于项目的协同过滤推荐的基本原理也是类似的，只是说它使用所有用户对物品或者信息的偏好，发现物品和物品之间的相似度，然后根据用户的历史偏好信息，将类似的物品推荐给用户。

基于模型的协同过滤推荐

基于模型的协同过滤推荐就是基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测，计算推荐。

协同过滤小结

优点:

a. 它不需要对物品或者用户进行严格的建模，而且不要求物品的描述是机器可理解的，所以这种方法也是领域无关的。

b. 这种方法计算出来的推荐是开放的，可以共用他人的经验，很好的支持用户发现潜在的兴趣偏好

缺点:

a. 方法的核心是基于历史数据，所以对新物品和新用户都有“冷启动”的问题。

b. 推荐的效果依赖于用户历史偏好数据的多少和准确性。

c. 在大部分的实现中，用户历史偏好是用稀疏矩阵进行存储的，而稀疏矩阵上的计算有些明显的问题，包括可能少部分人的错误偏好会对推荐的准确度有很大的影响等等。

d. 对于一些特殊品味的用户不能给予很好的推荐。

e. 由于以历史数据为基础，抓取和建模用户的偏好后，很难修改或者根据用户的使用演变，从而导致这个方法不够灵活。



k近邻算法KNN

算法原理

简单地说，k近邻算法就是采用测量不同特征值之间的距离方法进行分类。

优点：精度高，对异常值不敏感，无数据输入假定。

缺点：计算复杂度高，空间复杂度高。

适用数据类型：数值型和标称型。





伪代码如下：

1.计算样本点与当前点的距离

2.取出与当前点距离最小的前k个点

3.计算取出的k个点的类别出现频率

4.将出现频率最高的类别作为当前点的类别
